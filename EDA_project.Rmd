---
title: "EDA Project"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
#setwd("/Users/natewagner/Downloads/the-movies-dataset")
```


Load packages:
```{r}
library(readr)
library(tidyverse)
library(lubridate)
library(jsonlite)
library(data.table)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(knitr)
#install.packages("gt")
#install.packages("tm")
#installed.packages("SnowballC")
#install.packages("wordcloud")
#install.packages("RColorBrewer")
```


Read data:
```{r}
movies <- read_csv("movies_metadata.csv")
credits <- read_csv("credits.csv")
keywords <- read_csv("keywords.csv")
links <- read_csv("links.csv")
ratings <- read_csv("ratings.csv")
```


Check NAs:
```{r}
str(movies)

sum.NA.total <- movies %>%
  select(everything()) %>%
  summarise_all(funs(sum(is.na(.))))

PropOfNA.total <- (sum.NA.total/nrow(movies))*100
SumNA.prop <- PropOfNA.total %>% map_dbl(sum)
kable(SumNA.prop, format = "markdown")

```


# EDA
```{r}
# freq table for # movies per year:
movies_byYear <- movies %>% 
  group_by(year(release_date)) %>% 
  count()

# barplot of # movies per year:
ggplot(movies_byYear, aes(`year(release_date)`, n)) + 
  geom_bar(stat="identity", width = 0.5, fill = "tomato2") + 
  labs(title = "Number of Movies per Year", x = "Year", y = "Number of Movies") + 
  theme_classic()

summary(year(movies$release_date))



movies$budget[movies$budget == 0] <- NA 
summary(movies$budget)

movies <- mutate(movies, gross = revenue - budget)

movies %>% filter(title == "Titanic") %>% select(c(release_date, title))
```


# What affects movie ratings?

There are a couple extreme outliers with average ratings of 0 and vote counts greater than 30. For simplicity and to get a better representation of our scatterplots we are going to exclude these two points. 
```{r}
summary(movies$vote_average)
summary(movies$vote_count)

movies %>% filter(vote_average < .50 & vote_average > 0) %>% select(c(title, vote_average, vote_count))
movies <- movies %>% filter(vote_average > 0.50)
```


Correlation Matrix
```{r}
# select numeric columns & add year column:
movies <- mutate(movies, year = year(movies$release_date))

num.cols <- dplyr::select_if(movies, is.numeric)

# compute cor from all numeric columns:
cor.matrix <- round(cor(num.cols, use = "pairwise"), 2)

# melt the cor matrix:
cor.matrix.melt <- melt(cor.matrix)
```


Looking at the correlation matrix, there doesn't seem to be any real strong linear relationships between any of these variables and average movie ratings. But perhaps there are some non-linear relationships. 
```{r fig2, fig.height = 15, fig.width = 20, fig.align = "center"}
# plot cor matrix
ggplot(data = cor.matrix.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() + theme(axis.text.x = element_text(angle=65, vjust=0.6))

kable(cor.matrix, format = "markdown")
```


We are going to assume that to have an unbiased estimate of a movie's true average rating, there must be at least 30 votes, and we find 12,421 movies that meet this criteria. However, a potential source of bias with this approach is that it could be that movies with very low vote totals, are not very good movies to begin with and aren't popular. Thus, that could be why they have low vote counts and possible low average ratings.  
```{r}
movies %>% 
  filter(vote_count >= 30) %>%
  count()
```


Are low vote counts associated with lower average ratings?

It could be hard to see the actual trend with the extreme values of vote count.
```{r}
ggplot(movies %>% filter(vote_count > 0), aes(vote_count, vote_average)) +
  geom_point(alpha = 0.05) + 
  geom_smooth() + 
  labs(title = "Vote Count vs Vote Average", x = "Vote Count", y = "Vote Average") +
  theme_bw()
```


Even with removing the extreme values of vote count, it's still hard to see much of a releationship between vote count and vote average. 
```{r}
ggplot(movies %>% filter(vote_count > 0 & vote_count < 31), aes(vote_count, vote_average)) +
  geom_point(alpha = 0.05) +
  geom_smooth() +
  labs(title = "Vote Count vs Vote Average", subtitle = "n < 31", x = "Vote Count", y = "Vote Average") +
  theme_bw()
```


Remove movies with vote count less than 30
```{r}
movies <- movies %>% filter(vote_count >= 30)
```


How has average vote changed over time?

It seems there is a slight negative relationship between average vote and when the movie was released. 
```{r}
ggplot(movies, aes(year, vote_average)) +
  geom_point(alpha = 0.1) +
  geom_smooth() +
  labs(title = "Release Date vs Vote Average", x = "Year", y = "Vote Average") + 
  theme_bw()
```


Association between runtime and average ratings:

There are 71 movies with runtime equal to zero.
```{r}
summary(movies$runtime)

movies %>%
  filter(runtime == 0) %>%
  count()
```

Hard to see association with the extreme values of runtime. 
```{r}
ggplot(movies %>% filter(runtime > 0), aes(runtime, vote_average)) +
  geom_point(alpha = 0.1) +
  geom_smooth() +
  labs(title = "Runtime vs Vote Average", x = "Runtime", y = "Vote Average") + 
  theme_bw()
```


Even with the removal of runtime, it's still hard to see much of an association between runtime and average movie rating. 
```{r}
lower <- quantile(movies$runtime, 0.25, na.rm = TRUE) - 1.5 * IQR(movies$runtime, na.rm =T)
upper <- quantile(movies$runtime, 0.75, na.rm = TRUE) + 1.5 * IQR(movies$runtime, na.rm =T)
ggplot(movies %>% filter(runtime >= lower & runtime <= upper), aes(runtime, vote_average)) +
  geom_point(alpha = 0.1) +
  geom_smooth() +
  labs(title = "Runtime vs Vote Average", x = "Runtime", y = "Vote Average") +
  theme_bw()
```


Revenue vs. Movie Ratings
```{r}
summary(movies$revenue)

movies %>%
  filter(revenue > 1000000000) %>%
  count()
```


```{r}
ggplot(movies %>% filter(budget > 0), aes(revenue, vote_average)) +
  geom_point(alpha = 0.1)
```


Popularity vs Average Vote:

Not sure how popularity is measured.
```{r}
summary(movies$popularity)
```

Even with the removal of extreme outliers, there seems to be no relationship between movie popularity and average movie rating.
```{r}
lower <- quantile(movies$popularity, 0.25, na.rm = TRUE) - 1.5 * IQR(movies$popularity, na.rm =T)
upper <- quantile(movies$popularity, 0.75, na.rm = TRUE) + 1.5 * IQR(movies$popularity, na.rm =T)

ggplot(movies %>% filter(popularity >= lower & popularity < upper), aes(popularity, vote_average)) + geom_point(alpha = 0.1) +
  geom_smooth() +
  labs(title = "Popularity vs Vote Average", x = "Popularity", y = "Vote Average") +
  theme_bw()
```






# to unpack genres column:
```{r eval=FALSE, include=FALSE}
metadata <- fread("movies_metadata.csv", select=c('adult', 'genres', 'release_date', 'original_language', 'original_title', 'id', 'imdb_id'), fill=T)
metadata <- metadata[!is.na(as.integer(id))]

# Replace singles quote in 'genre' so 'fromJSON' function can parse it
metadata <- metadata[, genre := gsub("\'","\"", metadata$genre)]
# Remove redundant fields
metadata[, c('original_language', 'adult', "imdb_id") := NULL]

genres <- metadata[, unlist(lapply(metadata$genre, fromJSON), recursive=F)['name'], by=id]
sorted.genres <- genres[, .N, by=name][order(-N)]
sorted.genres[1:20]

genres[, dummy := 1]

encoded.genres <- dcast(na.omit(genres[,.SD[1:3], by=id])[name %in% sorted.genres[1:20, name]], 
                        id ~  name, value.var='dummy', fill=0)
data.w.genre <- merge(encoded.genres,
      metadata[, .(id, original_title, release_date)], 
      all.x=T, by="id")

nrow(data.w.genre)
nrow(movies)
```



```{r eval=FALSE, include=FALSE}
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator 
#install.packages("RColorBrewer") # color palettes
# install.packages(c("tm", "SnowballC", "wordcloud", "RColorBrewer", "RCurl", "XML"))
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
keywordsDS <- data.frame(keywords)
# Create the corpus of data
keywordsDS.corpus <- Corpus(VectorSource(keywordsDS$keywords))
# Cleaning dat. This includes removing stopwords, numbers, whitespace, etc. and converting the corpus into a plain text document.
keywordsDS.Clean <-tm_map(keywordsDS.corpus, PlainTextDocument)
keywordsDS.Clean<-tm_map(keywordsDS.corpus,tolower)
keywordsDS.Clean <-tm_map(keywordsDS.Clean,removeNumbers)
keywordsDS.Clean <-tm_map(keywordsDS.Clean,removeWords, stopwords("english"))
keywordsDS.Clean <-tm_map(keywordsDS.Clean,removeWords, c("name"))
keywordsDS.Clean <-tm_map(keywordsDS.Clean,removePunctuation)
keywordsDS.Clean <-tm_map(keywordsDS.Clean,stripWhitespace)
keywordsDS.Clean <-tm_map(keywordsDS.Clean,stemDocument)
wordcloud(words = keywordsDS.Clean, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
# Build a term-document matrix. Document matrix is a table containing the frequency of the words. Column names are words and row names are documents.
dtm <- TermDocumentMatrix(keywordsDS.Clean)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Analyze the association between frequent terms (i.e., terms which correlate)
findAssocs(dtm, terms = "love", corlimit = 0.3)
findAssocs(dtm, terms = "music", corlimit = 0.3)
findAssocs(dtm, terms = "war", corlimit = 0.3)
findAssocs(dtm, terms = "world", corlimit = 0.3)
# The frequency of the first 10 frequent words plotted
barplot(d[1:20,]$freq, 
        las = 2, 
        names.arg = d[1:20,]$word,
        col ="lightyellow", 
        main ="Most frequent words",
        ylab = "Word frequencies")




```



